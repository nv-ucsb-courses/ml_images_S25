{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/nv-classes/b00007a21a27f4bad79f9f570212f7c1/copy-of-copy-of-t81_558_class_03_2_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oX6K_-JCtiL"
      },
      "source": [
        "# Introduction to TensorFlow and Keras\n",
        "Adapted from a Jupyter notebook by [Jeff Heaton](https://sites.wustl.edu/jeffheaton/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zg-zNx0CtiN"
      },
      "source": [
        "TensorFlow is an open-source software library for deep learning. TensorFlow was originally developed by the Google Brain team for Google's research and production purposes and later released under the Apache 2.0 open source license on November 9, 2015.\n",
        "\n",
        "* [TensorFlow Homepage](https://www.tensorflow.org/)\n",
        "* [TensorFlow GitHib](https://github.com/tensorflow/tensorflow)\n",
        "* [TensorFlow Google Groups Support](https://groups.google.com/forum/#!forum/tensorflow)\n",
        "* [TensorFlow Google Groups Developer Discussion](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)\n",
        "* [TensorFlow FAQ](https://www.tensorflow.org/resources/faq)\n",
        "\n",
        "## Why TensorFlow\n",
        "\n",
        "* Supported by Google\n",
        "* _Works out of the box in Google Colab_\n",
        "* Works well on Windows, Linux, and Mac\n",
        "* Excellent GPU support\n",
        "* Python-based\n",
        "\n",
        "## Deep Learning Tools\n",
        "TensorFlow is not the only game in town. The biggest competitor to TensorFlow/Keras is PyTorch.\n",
        "\n",
        "* **[TensorFlow](https://www.tensorflow.org/)** - Google's deep learning API.  The focus of this class, along with Keras.\n",
        "\n",
        "* **[PyTorch](https://pytorch.org/)** - PyTorch is an open-source machine learning library based on the Torch library, used for computer vision and natural language applications processing. Facebook's AI Research lab primarily develops PyTorch.\n",
        "\n",
        "* **[Keras](https://keras.io/)** - Acts as a higher-level frontend to Tensorflow (and now, in its latest incarnation, also for PyTorch and [Jax](https://docs.jax.dev/en/latest/)).\n",
        "\n",
        "Generally, Keras requires significantly fewer lines of code to perform deep learning applications. However, if you are creating entirely new neural network structures in a research setting, direct use of PyTorch or TensorFlow can provide easier access to some of the low-level internals of deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMVIS9pBCtiQ"
      },
      "source": [
        "## Using TensorFlow Directly\n",
        "\n",
        "We will mostly focus on Keras (using TensorFlow as a backend), which allows us to specify the number of hidden layers and create the neural network easily. TensorFlow is a low-level mathematics API, similar to [Numpy](http://www.numpy.org/). However, unlike Numpy, TensorFlow is built for deep learning. TensorFlow compiles these compute graphs into highly efficient C++/[CUDA](https://en.wikipedia.org/wiki/CUDA) code running on GPUs. In the next section, we will see a TensorFlow example that has nothing to do with neural networks.\n",
        "\n",
        "### Mandelbrot in TensorFlow\n",
        "\n",
        "Next, we examine an example where we use TensorFlow directly. The example shows that TensoFlow does not only provide neural networks, but can also provide other mathematical. The code in the next section renders a [Mandelbrot set](https://en.wikipedia.org/wiki/Mandelbrot_set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4ll1OLwCtiQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import PIL.Image\n",
        "from io import BytesIO\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def render(a):\n",
        "  a_cyclic = (a*0.3).reshape(list(a.shape)+[1])\n",
        "  img = np.concatenate([10+20*np.cos(a_cyclic),\n",
        "                        30+50*np.sin(a_cyclic),\n",
        "                        155-80*np.cos(a_cyclic)], 2)\n",
        "  img[a==a.max()] = 0\n",
        "  a = img\n",
        "  a = np.uint8(np.clip(a, 0, 255))\n",
        "  f = BytesIO()\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "# difine functions for mandelbrot generation with tf routines\n",
        "def mandelbrot_helper(grid_c, current_values, counts,cycles):\n",
        "\n",
        "  for i in range(cycles):\n",
        "    temp = current_values*current_values + grid_c\n",
        "    not_diverged = tf.abs(temp) < 4\n",
        "    current_values.assign(temp),\n",
        "    counts.assign_add(tf.cast(not_diverged, tf.float32))\n",
        "\n",
        "def mandelbrot(render_size,center,zoom,cycles):\n",
        "  f = zoom/render_size[0]\n",
        "  real_start = center[0]-(render_size[0]/2)*f\n",
        "  real_end = real_start + render_size[0]*f\n",
        "  imag_start = center[1]-(render_size[1]/2)*f\n",
        "  imag_end = imag_start + render_size[1]*f\n",
        "\n",
        "  real_range = tf.range(real_start,real_end,f,dtype=tf.float64)\n",
        "  imag_range = tf.range(imag_start,imag_end,f,dtype=tf.float64)\n",
        "  real, imag = tf.meshgrid(real_range,imag_range)\n",
        "  grid_c = tf.constant(tf.complex(real, imag))\n",
        "  current_values = tf.Variable(grid_c)\n",
        "  counts = tf.Variable(tf.zeros_like(grid_c, tf.float32))\n",
        "\n",
        "  mandelbrot_helper(grid_c, current_values,counts,cycles)\n",
        "  return counts.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUIw04vRCmc1"
      },
      "source": [
        "With the above code defined, we can now calculate and render a Mandlebrot plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MZLfaYpCaO2"
      },
      "outputs": [],
      "source": [
        "counts = mandelbrot(\n",
        "    #render_size=(3840,2160), # 4K\n",
        "    #render_size=(1920,1080), # HD\n",
        "    render_size=(640,480),\n",
        "    center=(-0.5,0),\n",
        "    zoom=4,\n",
        "    cycles=200\n",
        ")\n",
        "img = render(counts)\n",
        "print(img.size)\n",
        "img\n",
        "\n",
        "#img.save(\"test.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeWypKgGCtiR"
      },
      "source": [
        "Mandlebrot rendering programs are both simple and infinitely complex at the same time. This view shows the entire Mandlebrot universe simultaneously, as a view completely zoomed out. However, if you zoom in on any non-black portion of the plot, you will find infinite hidden complexity.\n",
        "\n",
        "## Introduction to Keras\n",
        "\n",
        "[Keras](https://keras.io/) is a layer on top of TensorFlow that makes it much easier to create neural networks. Rather than define every operation, you set the individual layers of the network with a much more high-level API. Unless you are researching entirely new structures of deep neural networks, it is unlikely that you need to program TensorFlow directly.  \n",
        "\n",
        "## Simple TensorFlow Regression: MPG\n",
        "\n",
        "This example shows how to encode an MPG dataset for regression and predict values. We will predict the miles per gallon (MPG) for a car based on the car's weight, cylinders, engine size, and other features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmb3cFrUCtiR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\",\n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "cars = df['name']\n",
        "\n",
        "# Handle missing value\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "\n",
        "# Pandas to Numpy\n",
        "x_mpg = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
        "       'acceleration', 'year', 'origin']].values\n",
        "y_mpg = df['mpg'].values # regression\n",
        "\n",
        "# Build the neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(25, activation='relu')) # Hidden 1\n",
        "model.add(Dense(10, activation='relu')) # Hidden 2\n",
        "model.add(Dense(1)) # Output\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x_mpg,y_mpg,epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82O2rvqiCtiR"
      },
      "source": [
        "## Neural Network Hyperparameters\n",
        "\n",
        "The neural network above contains four layers. The first layer is the input layer and does not show up explicitly. It is automatically generated from the input and provides one node (neuron)for every column in the data set (including dummy variables, if present).\n",
        "\n",
        "There are also two hidden layers, with 25 and 10 neurons each. You might be wondering how the programmer chose these numbers. Selecting a hidden neuron structure is one of the most common questions about neural networks. Unfortunately, there is no right answer. These are hyperparameters. They are settings that can affect neural network performance, yet there are no clearly defined means of setting them.\n",
        "\n",
        "In general, more hidden neurons mean more capability to fit complex problems. However, too many neurons can lead to overfitting and lengthy training times. Too few can lead to underfitting the problem and will sacrifice accuracy. Also, how many layers you have is another hyperparameter. In general, more layers allow the neural network to perform more of its feature engineering and data preprocessing. But this also comes at the expense of training times and the risk of overfitting. In general, you will see that neuron counts start larger near the input layer and tend to shrink towards the output layer in a triangular fashion.\n",
        "\n",
        "\n",
        "\n",
        "## Regression Prediction\n",
        "\n",
        "Next, we will perform actual predictions. The program assigns these predictions to the **pred** variable. These are all MPG predictions from the neural network. Notice that this is a 2D array? You can always see the dimensions of what Keras returns by printing out **pred.shape**. Neural networks can return multiple values, so the result is always an array. Here the neural network only returns one value per prediction (there are 398 cars, so 398 predictions). However, a 2D range is needed because the neural network has the potential of returning more than one value.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbErLyX_CtiR"
      },
      "outputs": [],
      "source": [
        "pred_mpg = model.predict(x_mpg)\n",
        "print(f\"Shape: {pred_mpg.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk_lWnScCtiR"
      },
      "source": [
        "We would like to see how good these predictions are.  We know the correct MPG for each car so we can measure how close the neural network was."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrY0Vs9oCtiR"
      },
      "outputs": [],
      "source": [
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "score = np.sqrt(metrics.mean_squared_error(pred_mpg,y_mpg))\n",
        "print(f\"Final score (RMSD): {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl7hv8NnCtiS"
      },
      "outputs": [],
      "source": [
        "# Sample predictions\n",
        "for i in range(0,200,20):\n",
        "    print(f\"{i+1}. Car name: {cars[i]}, MPG: {y_mpg[i]}, \"\n",
        "          + f\"predicted MPG: {pred_mpg[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_dpgVtfCtiS"
      },
      "source": [
        "## Simple Classification: Iris dataset (ML \"Hello World\")\n",
        "\n",
        "We want to report a classification as their percent confidence in each class rather than just the top label.\n",
        "\n",
        "We previously saw how to train a neural network to predict the MPG of a car. Based on four measurements, we will now see how to predict a class, in this case the type of iris flower. The code to classify iris flowers is similar to MPG; however, there are several important differences:\n",
        "\n",
        "* The output neuron count matches the number of classes (in the case of Iris, 3).\n",
        "* The *Softmax activation function* is utilized by the output layer and the loss function we use is *cross entropy*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLp65T9JCtiS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\",\n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "# Convert to numpy - Classes/Labels\n",
        "x_iris = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
        "dummies = pd.get_dummies(df['species']) # Classification\n",
        "species = dummies.columns\n",
        "y_iris = dummies.values\n",
        "\n",
        "\n",
        "# Build neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, activation='relu')) # Hidden 1\n",
        "model.add(Dense(25, activation='relu')) # Hidden 2\n",
        "model.add(Dense(y_iris.shape[1],activation='softmax')) # Output\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(x_iris,y_iris,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPM30WdBCtiS"
      },
      "outputs": [],
      "source": [
        "# Print out number of species found:\n",
        "print(species)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM8-xyDxCtiS"
      },
      "source": [
        "Now that we have a trained neural network, we would like to use it. Exactly like before, we will generate predictions. This time, instead of one value (MPG prediction), three values are generated for each of the 150 iris flowers. These correspond to the probability of belonging to the three types of iris (Iris-setosa, Iris-versicolor, and Iris-virginica).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YzlVpw-CtiS"
      },
      "outputs": [],
      "source": [
        "pred_iris = model.predict(x_iris)\n",
        "\n",
        "print(f\"Shape: {pred_iris.shape}\")\n",
        "print(species)\n",
        "\n",
        "np.set_printoptions(formatter={'float': lambda x: format(x, '.3f')})\n",
        "print(pred_iris[0:150:15])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDC7hxqECtiS"
      },
      "source": [
        "Usually, the program considers the column with the highest prediction to be the prediction of the neural network.  It is easy to convert the predictions to the expected iris species.  The argmax function finds the index of the maximum prediction for each row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "367mbx_PCtiT"
      },
      "outputs": [],
      "source": [
        "predict_classes = np.argmax(pred_iris,axis=1)\n",
        "expected_classes = np.argmax(y_iris,axis=1)\n",
        "print(f\"Predictions:\\n {predict_classes}\")\n",
        "print(f\"Expected:\\n {expected_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljez1ZRACtiT"
      },
      "source": [
        "Accuracy might be a more easily understood error metric.  It is essentially a test score.  For all of the iris predictions, what percent were correct?  The downside is it does not consider how confident the neural network was in each prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zth2S2OcCtiT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "correct = accuracy_score(expected_classes,predict_classes)\n",
        "print(f\"Accuracy: {correct}\")"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "class_keras_tf.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}